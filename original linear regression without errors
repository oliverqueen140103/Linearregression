# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Load the dataset (assuming it's in a CSV file)
# Replace 'your_dataset.csv' with the actual file name
insurance_data = pd.read_csv(r"C:\Users\Oliver\Downloads\insurance.csv")

# Display the first few rows of the dataset to understand its structure
print(insurance_data.head())

# Extract features and target variable
X = insurance_data[['age', 'sex', 'bmi', 'smoker', 'region', 'children']]
y = insurance_data['charges']

sns.lmplot(x='bmi',y='charges',data=insurance_data,aspect=2,height=6)
plt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')
plt.ylabel('Insurance Charges: as Dependent variable')
plt.title('Charge Vs BMI');

# Convert categorical variables to numerical using one-hot encoding
encoder = OneHotEncoder(drop='first', sparse=False)
X_encoded = pd.DataFrame(encoder.fit_transform(X[['sex', 'smoker', 'region']]))
X_encoded.columns = encoder.get_feature_names_out(['sex', 'smoker', 'region'])
X = pd.concat([X.drop(['sex', 'smoker', 'region'], axis=1), X_encoded], axis=1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model performance
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Display the model coefficients and intercept
print('Coefficients:', model.coef_)
print('Intercept:', model.intercept_)

# Plot the predicted charges vs. actual charges with the overall linear regression line
plt.scatter(y_test, y_pred, label='Data Points')
plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='red', linestyle='--', linewidth=2, label='Linear Regression Line')
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.title('Actual vs. Predicted Charges with Linear Regression Line')
plt.legend()
plt.show()

# Add a figure for overall linear regression with charges vs. BMI
plt.figure()

# Scatter plot for actual data
plt.scatter(insurance_data['bmi'], insurance_data['charges'], alpha=0.5, label='Actual Data')

# Linear regression line for overall data
overall_model = LinearRegression()
overall_model.fit(X[['bmi']], y)  # Fit the model on the entire dataset
overall_pred = overall_model.predict(X[['bmi']])

# Plot the linear regression line
plt.plot(X['bmi'], overall_pred, color='red', linewidth=2, label='Overall Linear Regression')

plt.xlabel('BMI')
plt.ylabel('Charges')
plt.title('Overall Linear Regression: Charges vs. BMI')
plt.legend()
plt.show()


X_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]
X_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]

# Step2: build model
theta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train))

# The parameters for linear regression model
parameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]
columns = ['intersect:x_0=1'] + list(X.columns.values)
parameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})

# Scikit Learn module
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.
#Parameter
sk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)
parameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))
parameter_df
# Normal equation
y_pred_norm =  np.matmul(X_test_0,theta)

#Evaluvation: MSE
J_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]

# R_square
sse = np.sum((y_pred_norm - y_test)**2)
sst = np.sum((y_test - y_test.mean())**2)
R_square = 1 - (sse/sst)
print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)
print('R square obtain for normal equation method is :',R_square)
# sklearn regression module
y_pred_sk = lin_reg.predict(X_test)

#Evaluvation: MSE
from sklearn.metrics import mean_squared_error
J_mse_sk = mean_squared_error(y_pred_sk, y_test)

# Create a DataFrame to use with seaborn
result_df = pd.DataFrame({'Actual Charges': y_test, 'Predicted Charges': y_pred_sk})

# Scatter plot using seaborn
sns.scatterplot(x='Actual Charges', y='Predicted Charges', data=result_df, color='r')

# Additional customization if needed
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.title('Actual vs. Predicted Charges')
plt.show()

# Plot the predicted charges vs. actual charges with the overall linear regression line
plt.scatter(y_test, y_pred, label='Data Points')
plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='red', linestyle='--', linewidth=2, label='Linear Regression Line')
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.title('Actual vs. Predicted Charges with Linear Regression Line')
plt.legend()
plt.show()

# R_square
R_square_sk = lin_reg.score(X_test,y_test)
print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)
print('R square obtain for scikit learn library is :',R_square_sk)

# Check for Linearity
f = plt.figure(figsize=(14,5))
ax = f.add_subplot(121)
#sns.scatterplot(data=insurance_data, x="y_test", y="y_pred_sk", ax=ax, hue="blue")
# Assuming y_test and y_pred_sk are NumPy arrays or pandas Series
result_df = pd.DataFrame({'Actual Charges': y_test, 'Predicted Charges': y_pred_sk})

# Scatter plot using seaborn
sns.displot(data=result_df, x='Actual Charges', y='Predicted Charges', color='r')

#sns.scatterplot(y_test,y_pred_sk,ax=ax,color='r')
ax.set_title('Check for Linearity:\n Actual Vs Predicted value')

# Check for Residual normality & mean
ax = f.add_subplot(122)
sns.histplot((y_test - y_pred_sk),ax=ax,color='b')
ax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')
ax.set_title('Check for Residual normality & mean: \n Residual eror');

f,ax = plt.subplots(1,2,figsize=(14,6))
import scipy as sp
_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])
ax[0].set_title('Check for Multivariate Normality: \nQ-Q Plot')

#Check for Homoscedasticity
sns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r')
ax[1].set_title('Check for Homoscedasticity: \nResidual Vs Predicted');

# Check for Multicollinearity
#Variance Inflation Factor
VIF = 1/(1- R_square_sk)
